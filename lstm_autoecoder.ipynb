{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoderAnomalyDetector:\n",
    "    \"\"\"\n",
    "    LSTM Autoencoder for Anomaly Detection in Time Series Data\n",
    "    \n",
    "    This class provides a complete workflow for training an LSTM Autoencoder\n",
    "    to detect anomalies in sequential data, with built-in preprocessing, \n",
    "    training, and anomaly detection capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_timesteps=10, n_features=18):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM Autoencoder Anomaly Detector\n",
    "        \n",
    "        Args:\n",
    "            n_timesteps (int): Number of time steps in input sequence\n",
    "            n_features (int): Number of features in each time step\n",
    "        \"\"\"\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_features = n_features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.anomaly_threshold = None\n",
    "    \n",
    "    def _create_sequences(self, data, n_timesteps):\n",
    "        \"\"\"\n",
    "        Convert flat data into sequences for LSTM\n",
    "        \n",
    "        Args:\n",
    "            data (np.ndarray): Input data array\n",
    "            n_timesteps (int): Number of time steps in sequence\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Sequences of data\n",
    "        \"\"\"\n",
    "        if len(data) < n_timesteps:\n",
    "            raise ValueError(f\"Not enough data ({len(data)} samples) to create sequences of length {n_timesteps}.\")\n",
    "        \n",
    "        sequences = []\n",
    "        for i in range(len(data) - n_timesteps + 1):\n",
    "            sequences.append(data[i:i + n_timesteps])\n",
    "        \n",
    "        return np.array(sequences)\n",
    "    \n",
    "    def _build_lstm_autoencoder(self):\n",
    "        \"\"\"\n",
    "        Build LSTM Autoencoder model architecture\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled LSTM Autoencoder model\n",
    "        \"\"\"\n",
    "        input_shape = (self.n_timesteps, self.n_features)\n",
    "        \n",
    "        inputs = keras.Input(shape=input_shape)\n",
    "        \n",
    "        # Encoder\n",
    "        x = layers.LSTM(units=24, activation='tanh', return_sequences=True, name='LSTM_Enc_1')(inputs)\n",
    "        encoded = layers.LSTM(units=6, activation='tanh', return_sequences=False, name='LSTM_Enc_2_Bottleneck')(x)\n",
    "        \n",
    "        # Decoder\n",
    "        z = layers.RepeatVector(self.n_timesteps, name='RepeatVector')(encoded)\n",
    "        x = layers.LSTM(units=6, activation='tanh', return_sequences=True, name='LSTM_Dec_1')(z)\n",
    "        x = layers.LSTM(units=24, activation='tanh', return_sequences=True, name='LSTM_Dec_2')(x)\n",
    "        \n",
    "        # Output Layer\n",
    "        decoded = layers.TimeDistributed(layers.Dense(units=self.n_features, activation='linear'), \n",
    "                                        name='Output_Dense')(x)\n",
    "        \n",
    "        autoencoder = keras.Model(inputs, decoded, name=\"LSTM_Autoencoder\")\n",
    "        autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mae')\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"\n",
    "        Preprocess input data by scaling and creating sequences\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame or np.ndarray): Input data\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed sequences\n",
    "        \"\"\"\n",
    "        # Ensure data is numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.values\n",
    "        \n",
    "        # Scale data\n",
    "        scaled_data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Create sequences\n",
    "        return self._create_sequences(scaled_data, self.n_timesteps)\n",
    "    \n",
    "    def train(self, data, epochs=50, batch_size=64, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Train the LSTM Autoencoder\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame or np.ndarray): Training data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "            validation_split (float): Proportion of data to use for validation\n",
    "        \"\"\"\n",
    "        # Preprocess data\n",
    "        sequences = self.preprocess_data(data)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(sequences, test_size=validation_split, shuffle=False)\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self._build_lstm_autoencoder()\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val),\n",
    "            callbacks=[early_stopping],\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        val_mae_loss = np.mean(np.abs(\n",
    "            self.model.predict(X_val) - X_val\n",
    "        ), axis=(1, 2))\n",
    "        \n",
    "        self.anomaly_threshold = np.mean(val_mae_loss) + 3.0 * np.std(val_mae_loss)\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history.history['loss'], label='Training MAE Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation MAE Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Mean Absolute Error (MAE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def detect_anomalies(self, data):\n",
    "        \"\"\"\n",
    "        Detect anomalies in input data\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame or np.ndarray): Data to check for anomalies\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Boolean array indicating anomalies\n",
    "        \"\"\"\n",
    "        if self.model is None or self.anomaly_threshold is None:\n",
    "            raise ValueError(\"Model must be trained before detecting anomalies\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        sequences = self.preprocess_data(data)\n",
    "        \n",
    "        # Predict and calculate reconstruction error\n",
    "        predictions = self.model.predict(sequences)\n",
    "        mae_loss = np.mean(np.abs(predictions - sequences), axis=(1, 2))\n",
    "        \n",
    "        # Detect anomalies\n",
    "        return mae_loss > self.anomaly_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, feature_columns):\n",
    "    \"\"\"\n",
    "    Load and validate data from CSV\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to CSV file\n",
    "        feature_columns (list): List of feature column names\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Validated dataframe\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Verify columns\n",
    "        missing_cols = [col for col in feature_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        return df[feature_columns]\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NORMAL_DATA_PATH = '/normal_data.csv'\n",
    "ANOMALY_DATA_PATH = '/anomaly_data.csv'\n",
    "\n",
    "feature_columns = [\n",
    "    f'actual_q_{i}' for i in range(6)] + \\\n",
    "    [f'actual_qd_{i}' for i in range(6)] + \\\n",
    "    [f'actual_current_{i}' for i in range(6)]\n",
    "\n",
    "# Initialize and train anomaly detector\n",
    "detector = LSTMAutoencoderAnomalyDetector(\n",
    "    n_timesteps=10, \n",
    "    n_features=len(feature_columns)\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Load and train on normal data\n",
    "    normal_data = load_data(NORMAL_DATA_PATH, feature_columns)\n",
    "    detector.train(normal_data)\n",
    "    \n",
    "    # Optional: Test on anomaly data\n",
    "    if os.path.exists(ANOMALY_DATA_PATH):\n",
    "        anomaly_data = load_data(ANOMALY_DATA_PATH, feature_columns)\n",
    "        anomalies = detector.detect_anomalies(anomaly_data)\n",
    "        \n",
    "        print(f\"Detected {anomalies.sum()} anomalies out of {len(anomalies)} sequences\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01c5111a2c7b8b3e2e3f53415dc61224cf3e7919bb90c4d6f1d95d6d1e96e383"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
